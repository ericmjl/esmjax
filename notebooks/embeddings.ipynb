{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## More with embeddings: structural similarity\n",
    "\n",
    "Protein sequences can have similar folded structure (and in turn, similar function) but often fairly diverging sequences. This seemingly contradictory behavior arises from the principle of co-evolution in proteins: when proteins do mutate, they tend to do so in pairs or groups such that they still have similar structure, to be able to carry out the original function.\n",
    "\n",
    "A model purely looking at sequences would have very different embeddings for such protein pairs despite their similar structures. A model that can build an internal representation for the structure of a protein would be able to place embeddings close. This notebook briefly explores whether ESM-1b does so.\n",
    "\n",
    "### Data\n",
    "We first download the [SCOPe 2.07](https://scop.berkeley.edu/astral/ver=2.07) dataset, which contains sequence data for a diverse range of protein *domains* (not whole proteins), classified on structural features. We use the dataset with sequences having less than 40% similarity to each other."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!wget https://scop.berkeley.edu/downloads/scopeseq-2.07/astral-scopedom-seqres-gd-sel-gs-bib-40-2.07.fa -O ../data/scope207.fa"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-10-03 10:04:29--  https://scop.berkeley.edu/downloads/scopeseq-2.07/astral-scopedom-seqres-gd-sel-gs-bib-40-2.07.fa\n",
      "Resolving scop.berkeley.edu (scop.berkeley.edu)... 128.32.236.13\n",
      "Connecting to scop.berkeley.edu (scop.berkeley.edu)|128.32.236.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4026229 (3.8M)\n",
      "Saving to: ‘../data/scope207.fa’\n",
      "\n",
      "../data/scope207.fa 100%[===================>]   3.84M  8.77MB/s    in 0.4s    \n",
      "\n",
      "2021-10-03 10:04:29 (8.77 MB/s) - ‘../data/scope207.fa’ saved [4026229/4026229]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "# we load in the proteins\n",
    "records = list(SeqIO.parse(\"../data/scope207.fa\", \"fasta\"))\n",
    "\n",
    "# for each protein we seperate the name, sequence and structure label\n",
    "names = [record.name for record in records]\n",
    "sequences = [str(record.seq.upper()) for record in records]\n",
    "structure_label = [re.search(r\".[.]\\d*[.]\\d*[.]\\d*\", rec.description).group(0) for rec in records]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(names[0], sequences[0], structure_label[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d1dlwa_ SLFEQLGGQAAVQAVTAQFYANIQADATVATFFNGIDMPNQTNKTAAFLCAALGGPNAWTGRNLKEVHANMGVSNAQFTTVIGHLRSALTGAGVAAALVEQTVAVAETVRGDVVTV a.1.1.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this example, we only look at domains with made of alpha-helices, that is a structure label starting with `a`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# we zip up the records to make filtering easier\n",
    "prot_domains = zip(names, sequences, structure_label)\n",
    "\n",
    "filtered_prot_domains = filter(lambda x: x[2][0]=='a', prot_domains)\n",
    "filtered_prot_domains = filter(lambda x: len(x[1]) <= 510, filtered_prot_domains)\n",
    "names, sequences, structure_label = list(zip(*filtered_prot_domains))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(sequences)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2505"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Let's quickly set up inference across all accelerators (here, 8 TPU cores) to make quick work of embedding all 2505 protein domains. Similar to previously, we load in the model params and construct the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from esmjax import models, modelio, tokenize\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "params_dict = modelio.load_model(\"../data/esm1b.h5\")\n",
    "\n",
    "# Instead of calling jnp.numpy, which moves the array to the *first* device\n",
    "# we use `device_put_replicated` to send a copy of weights to *all* devices\n",
    "devices = jax.local_devices()\n",
    "distrib_params_dict = jax.tree_map(lambda x: jax.device_put_replicated(x, devices), params_dict)\n",
    "\n",
    "params = hk.data_structures.to_immutable_dict(distrib_params_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A quick note is that running `device_put_replicated` on an array adds a new first dimension, which represents the copies of the array across all devices. The array itself is now a `ShardedDeviceArray`, which is logically one array, but is physically split across all devices. For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(params['esm1b/embed']['embeddings'].shape, type(params['esm1b/embed']['embeddings']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8, 33, 1280) <class 'jax.interpreters.pxla.ShardedDeviceArray'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we just construct the model, except this time, instead of `jit`-ing it, we `pmap` it. JAX takes care of the rest from here and it's parallelized:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "esm1b_f = hk.transform(lambda x: models.ESM1b()(x))\n",
    "esm1b_f = hk.without_apply_rng(esm1b_f)\n",
    "esm1b_apply = jax.pmap(esm1b_f.apply)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Embeddings\n",
    "\n",
    "We use a batch size of 16 protein domains per TPU, so 128 proteins at a time. All TPUs must be passed in an array of the exact same size, and 2505 isn't divisible by 128, so we add in some extra blank sequences at the end that we'll discard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_DEVICES = len(jax.devices())\n",
    "\n",
    "superbatch_size = BATCH_SIZE * NUM_DEVICES\n",
    "num_sequences = len(sequences)\n",
    "num_sequences_padded = int(np.ceil(num_sequences / superbatch_size) * superbatch_size)\n",
    "\n",
    "sequences_list = list(sequences)\n",
    "sequences_list.extend([\"\"] * (num_sequences_padded - num_sequences))\n",
    "\n",
    "names_list = list(names)\n",
    "names_list.extend([None] * (num_sequences_padded - num_sequences))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the number of sequences is fairly small, it's fine to tokenize them all in one go."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "names_list, sequences_list, tokens = tokenize.convert(zip(names_list, sequences_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then embed all the sequences, in batches of size 16. This isn't particularly efficient (takes roughly a full minute on a TPU v2-8), and a better implementation of data infeed (such as by using `tf.data`) would allow it to scale to larger protein datasets. For this example we keep things relatively straightforward.\n",
    "\n",
    "Note that we sum the embeddings along the sequence dimensions; we'll average them later by dividing by the length of each protein. (This approach is fine, as the padding and other \"add-on\" tokens have their embeddings masked out to 0)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "embeddings = []\n",
    "\n",
    "max_seq_len = tokens.shape[-1]\n",
    "\n",
    "with jax.default_matmul_precision('float32'):\n",
    "    for i in range(0, num_sequences_padded, superbatch_size):\n",
    "        superbatch_tokens = tokens[i:i+superbatch_size, :]\n",
    "        batch_tokens = jnp.reshape(superbatch_tokens, (NUM_DEVICES, BATCH_SIZE, max_seq_len))\n",
    "\n",
    "        per_residue_embeddings = esm1b_apply(params, batch_tokens)[\"embeddings\"]\n",
    "        whole_prot_embeddings = per_residue_embeddings.sum(axis=-2)\n",
    "        embeddings.append(whole_prot_embeddings.reshape(superbatch_size, 1280))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concatenate the outputs of all the batches, and index out all the padding sequences, and convert the sum to a mean."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "embeddings = embeddings[:len(sequences), :]\n",
    "\n",
    "prot_lens = np.array([len(seq) for seq in sequences]).reshape(-1, 1)\n",
    "embeddings = embeddings / prot_lens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the embeddings\n",
    "\n",
    "Now that we have the embeddings, we'd ideally like to see how they relate to each other, especially for protein domains with similar structural features. We'll be using the [TriMap](https://github.com/eamid/trimap) algorithm, which preserves global structure (and not just local ones) better than UMAP or t-SNE. At this point our embeddings are just `ndarray`s, so you can in theory use any analysis tool of your choosing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import trimap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "low_d_embeddings = trimap.TRIMAP(n_dims=3).fit_transform(embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRIMAP(n_inliers=10, n_outliers=5, n_random=5, distance=euclidean, lr=1000.0, n_iters=400, weight_adj=500.0, apply_pca=True, opt_method=dbd, verbose=True, return_seq=False)\n",
      "running TriMap on 2505 points with dimension 1280\n",
      "pre-processing\n",
      "applied PCA\n",
      "found nearest neighbors\n",
      "sampled triplets\n",
      "running TriMap with dbd\n",
      "Iteration:  100, Loss: 76.999, Violated triplets: 0.0559\n",
      "Iteration:  200, Loss: 73.609, Violated triplets: 0.0534\n",
      "Iteration:  300, Loss: 71.704, Violated triplets: 0.0520\n",
      "Iteration:  400, Loss: 70.677, Violated triplets: 0.0513\n",
      "Elapsed time: 0:00:05.541536\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We color the points by the fold that they belong to; there are almost 300 fold types however (e.g. `a.1`, `a.2`..., you can learn more about what they correspond to [here](https://scop.berkeley.edu/sunid=46456&ver=2.07)), and a static plot would obscure a lot of important detail. The interactive plot saved below can be found [here](https://htmlpreview.github.io/?https://github.com/irhum/esm-jax/blob/main/notebooks/embeddings_vis.html). You can actually see how proteins of a similar fold pattern end up clustering together, even though their sequences are very different. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "folds = ['.'.join(label.split('.')[:2]) for label in structure_label]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import plotly.express as px"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "fig = px.scatter_3d(x=low_d_embeddings[:, 0], \n",
    "                    y=low_d_embeddings[:, 1],\n",
    "                    z=low_d_embeddings[:, 2],\n",
    "                    color=folds,\n",
    "                    size_max=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "fig.write_html('embeddings_vis.html')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('jaxtpu': conda)"
  },
  "interpreter": {
   "hash": "027cb59bf36f23e2c7549b483a8274087c9a2176e69b686967223011b6628fa4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}